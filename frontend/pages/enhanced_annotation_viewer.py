"""
Enhanced Annotation Viewer Page

Demonstrates the multi-source annotation rendering capabilities
"""

import io
import json
from typing import Dict, List

# Import the new annotation renderer
from components.annotation_renderer import (
    MultiSourceAnnotationRenderer,
    create_streamlit_annotation_interface,
)

import streamlit as st
from utils.api_client import UILabelingAPIClient


def show_enhanced_annotation_viewer():
    """Main function for the enhanced annotation viewer page"""

    st.header("ðŸŽ¨ Enhanced Annotation Viewer")
    st.markdown(
        """
        This tool allows you to visualize annotations from multiple sources on the same image:
        - **Manual Annotations**: Ground truth annotations created by users
        - **AI Predictions**: Annotations generated by AI models (LLM/MCP)
        - **Draft Annotations**: Temporary annotations being created
        - **Imported Annotations**: Annotations from external sources
        """
    )

    # Initialize API client
    @st.cache_resource
    def get_api_client():
        return UILabelingAPIClient()

    api_client = get_api_client()

    # Image selection
    st.subheader("ðŸ–¼ï¸ Select Image")

    try:
        images = api_client.list_images()

        if not images:
            st.warning("No images found. Please upload some images first.")
            return

        # Create image selection dropdown
        image_options = [f"{img['filename']} ({img['id'][:8]}...)" for img in images]

        selected_index = st.selectbox(
            "Choose an image to view annotations",
            range(len(image_options)),
            format_func=lambda x: image_options[x],
            help="Select an image to load and view its annotations",
        )

        selected_image = images[selected_index]
        selected_image_id = selected_image["id"]

        # Display selected image info
        st.info(
            f"**Selected**: {selected_image['filename']} | **Size**: {selected_image.get('width', 'Unknown')}Ã—{selected_image.get('height', 'Unknown')}px"
        )

    except Exception as e:
        st.error(f"âŒ Error loading images: {str(e)}")
        return

    # Load image data and annotations
    st.subheader("ðŸ“Š Annotation Sources")

    try:
        # Load image data
        image_content = api_client.get_image_file(selected_image_id)

        # Load different annotation sources
        manual_annotations = []
        ai_predictions = []
        draft_annotations = []
        imported_annotations = []

        # Source selection controls
        source_col1, source_col2, source_col3, source_col4 = st.columns(4)

        with source_col1:
            load_manual = st.checkbox("ðŸ“ Manual Annotations", value=True)
            if load_manual:
                try:
                    manual_annotations = api_client.get_annotations(selected_image_id)
                    st.success(f"âœ… {len(manual_annotations)} manual annotations")
                except Exception as e:
                    st.warning(f"âš ï¸ Could not load manual annotations: {str(e)}")

        with source_col2:
            load_ai = st.checkbox("ðŸ¤– AI Predictions", value=True)
            if load_ai:
                try:
                    predictions_data = api_client.get_predictions(selected_image_id)
                    if predictions_data and "predictions" in predictions_data:
                        ai_predictions = predictions_data["predictions"]
                        st.success(f"âœ… {len(ai_predictions)} AI predictions")
                    else:
                        st.info("â„¹ï¸ No AI predictions available")
                except Exception as e:
                    st.warning(f"âš ï¸ Could not load AI predictions: {str(e)}")

        with source_col3:
            load_draft = st.checkbox("âœï¸ Draft Annotations", value=False)
            if load_draft:
                # For demo purposes, create some sample draft annotations
                draft_annotations = [
                    {
                        "temp_id": "draft_1",
                        "bounding_box": {"x": 50, "y": 50, "width": 100, "height": 30},
                        "tag": "button",
                        "confidence": None,
                    },
                    {
                        "temp_id": "draft_2",
                        "bounding_box": {
                            "x": 200,
                            "y": 100,
                            "width": 150,
                            "height": 25,
                        },
                        "tag": None,  # Untagged draft
                        "confidence": None,
                    },
                ]
                st.info(f"â„¹ï¸ {len(draft_annotations)} draft annotations (demo data)")

        with source_col4:
            load_imported = st.checkbox("ðŸ“¥ Imported Annotations", value=False)
            if load_imported:
                # For demo purposes, allow user to upload JSON file
                uploaded_file = st.file_uploader(
                    "Upload annotations JSON",
                    type=["json"],
                    help="Upload a JSON file with annotation data",
                )

                if uploaded_file is not None:
                    try:
                        imported_data = json.load(uploaded_file)
                        # Expect format: [{"bounding_box": {...}, "tag": "...", ...}, ...]
                        if isinstance(imported_data, list):
                            imported_annotations = imported_data
                            st.success(
                                f"âœ… {len(imported_annotations)} imported annotations"
                            )
                        else:
                            st.error(
                                "âŒ Invalid JSON format. Expected array of annotations."
                            )
                    except json.JSONDecodeError:
                        st.error("âŒ Invalid JSON file")

        # Check if we have any annotations to display
        total_annotations = (
            len(manual_annotations)
            + len(ai_predictions)
            + len(draft_annotations)
            + len(imported_annotations)
        )

        if total_annotations == 0:
            st.warning(
                "âš ï¸ No annotations available for this image. Please select at least one annotation source or upload annotations."
            )

            # Show sample data option
            if st.button("ðŸ§ª Load Sample Annotations (for testing)"):
                # Create sample annotations for demonstration
                sample_manual = [
                    {
                        "bounding_box": {
                            "x": 100,
                            "y": 200,
                            "width": 120,
                            "height": 40,
                        },
                        "tag": "button",
                        "confidence": 1.0,
                    },
                    {
                        "bounding_box": {
                            "x": 300,
                            "y": 150,
                            "width": 180,
                            "height": 30,
                        },
                        "tag": "input",
                        "confidence": 1.0,
                    },
                ]

                sample_ai = [
                    {
                        "bounding_box": {
                            "x": 120,
                            "y": 220,
                            "width": 100,
                            "height": 35,
                        },
                        "tag": "button",
                        "confidence": 0.95,
                    },
                    {
                        "bounding_box": {
                            "x": 310,
                            "y": 160,
                            "width": 160,
                            "height": 25,
                        },
                        "tag": "input",
                        "confidence": 0.88,
                    },
                    {
                        "bounding_box": {"x": 500, "y": 180, "width": 80, "height": 25},
                        "tag": "dropdown",
                        "confidence": 0.75,
                    },
                ]

                # Store sample data in session state
                st.session_state["sample_manual"] = sample_manual
                st.session_state["sample_ai"] = sample_ai
                st.rerun()

            # Use sample data if available
            if "sample_manual" in st.session_state:
                manual_annotations = st.session_state["sample_manual"]
                ai_predictions = st.session_state["sample_ai"]
                st.info("â„¹ï¸ Using sample annotation data for demonstration")

        # Only proceed if we have annotations
        if (
            len(manual_annotations)
            + len(ai_predictions)
            + len(draft_annotations)
            + len(imported_annotations)
            > 0
        ):

            # Create the enhanced annotation interface
            st.markdown("---")
            create_streamlit_annotation_interface(
                image_data=image_content,
                image_id=selected_image_id,
                manual_annotations=manual_annotations,
                ai_predictions=ai_predictions,
                draft_annotations=draft_annotations,
                imported_annotations=imported_annotations,
            )

            # Additional analysis tools
            st.markdown("---")
            st.subheader("ðŸ” Advanced Analysis")

            analysis_tab1, analysis_tab2, analysis_tab3 = st.tabs(
                ["Overlap Analysis", "Confidence Distribution", "Tag Comparison"]
            )

            with analysis_tab1:
                st.markdown("**Annotation Overlap Analysis**")
                if manual_annotations and ai_predictions:
                    # Calculate overlap between manual and AI predictions
                    overlaps = calculate_annotation_overlaps(
                        manual_annotations, ai_predictions
                    )

                    if overlaps:
                        st.write(f"Found {len(overlaps)} overlapping annotations:")
                        for i, overlap in enumerate(overlaps[:5]):  # Show first 5
                            st.write(
                                f"â€¢ Manual #{overlap['manual_idx']} â†” AI #{overlap['ai_idx']} (IoU: {overlap['iou']:.3f})"
                            )
                    else:
                        st.info(
                            "No significant overlaps found between manual and AI annotations"
                        )
                else:
                    st.info(
                        "Need both manual annotations and AI predictions for overlap analysis"
                    )

            with analysis_tab2:
                st.markdown("**Confidence Score Distribution**")
                if ai_predictions:
                    confidences = [
                        pred.get("confidence", 0)
                        for pred in ai_predictions
                        if pred.get("confidence")
                    ]
                    if confidences:
                        import plotly.graph_objects as go

                        fig = go.Figure(data=[go.Histogram(x=confidences, nbinsx=20)])
                        fig.update_layout(
                            title="AI Prediction Confidence Distribution",
                            xaxis_title="Confidence Score",
                            yaxis_title="Count",
                            height=400,
                        )
                        st.plotly_chart(fig, use_container_width=True)

                        avg_confidence = sum(confidences) / len(confidences)
                        st.metric("Average Confidence", f"{avg_confidence:.3f}")
                    else:
                        st.info("No confidence scores available in AI predictions")
                else:
                    st.info("No AI predictions available for confidence analysis")

            with analysis_tab3:
                st.markdown("**Tag Distribution Comparison**")

                # Create tag comparison
                all_sources = []
                if manual_annotations:
                    all_sources.append(("Manual", manual_annotations))
                if ai_predictions:
                    all_sources.append(("AI Predictions", ai_predictions))
                if draft_annotations:
                    all_sources.append(("Draft", draft_annotations))
                if imported_annotations:
                    all_sources.append(("Imported", imported_annotations))

                if all_sources:
                    # Create comparison table
                    import pandas as pd

                    tag_data = {}
                    all_tags = set()

                    for source_name, annotations in all_sources:
                        tag_counts = {}
                        for ann in annotations:
                            tag = ann.get("tag", "untagged")
                            tag_counts[tag] = tag_counts.get(tag, 0) + 1
                            all_tags.add(tag)
                        tag_data[source_name] = tag_counts

                    # Create DataFrame
                    df_data = []
                    for tag in sorted(all_tags):
                        row = {"Tag": tag}
                        for source_name, _ in all_sources:
                            row[source_name] = tag_data[source_name].get(tag, 0)
                        df_data.append(row)

                    df = pd.DataFrame(df_data)
                    st.dataframe(df, use_container_width=True)
                else:
                    st.info("No annotation sources available for comparison")

    except Exception as e:
        st.error(f"âŒ Error loading image data: {str(e)}")
        with st.expander("ðŸ”§ Debug Information"):
            st.code(f"Error details: {str(e)}")


def calculate_annotation_overlaps(
    annotations1: List[Dict], annotations2: List[Dict], iou_threshold: float = 0.3
) -> List[Dict]:
    """Calculate overlaps between two sets of annotations"""

    def calculate_iou(bbox1: Dict, bbox2: Dict) -> float:
        """Calculate Intersection over Union (IoU) between two bounding boxes"""

        # Calculate intersection
        x1 = max(bbox1["x"], bbox2["x"])
        y1 = max(bbox1["y"], bbox2["y"])
        x2 = min(bbox1["x"] + bbox1["width"], bbox2["x"] + bbox2["width"])
        y2 = min(bbox1["y"] + bbox1["height"], bbox2["y"] + bbox2["height"])

        if x2 <= x1 or y2 <= y1:
            return 0.0

        intersection = (x2 - x1) * (y2 - y1)

        # Calculate union
        area1 = bbox1["width"] * bbox1["height"]
        area2 = bbox2["width"] * bbox2["height"]
        union = area1 + area2 - intersection

        return intersection / union if union > 0 else 0.0

    overlaps = []

    for i, ann1 in enumerate(annotations1):
        for j, ann2 in enumerate(annotations2):
            iou = calculate_iou(ann1["bounding_box"], ann2["bounding_box"])
            if iou >= iou_threshold:
                overlaps.append(
                    {
                        "manual_idx": i,
                        "ai_idx": j,
                        "iou": iou,
                        "manual_tag": ann1.get("tag"),
                        "ai_tag": ann2.get("tag"),
                        "tag_match": ann1.get("tag") == ann2.get("tag"),
                    }
                )

    return sorted(overlaps, key=lambda x: x["iou"], reverse=True)


if __name__ == "__main__":
    show_enhanced_annotation_viewer()
